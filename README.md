# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.

![alt text](https://github.com/himanshu004/AZMLND_Optimizing_a_Pipeline_in_Azure-Starter_Files/blob/master/images/automl-image.jpg)
<br>
The best performing model on the data using Azure's AutoML turned out to be VotingEnsemble classifier. It is based on the concept of ensembling machine learning that uses the idea of considering outputs from a variety of models, and then use them collectively to give a final result. With Azure's AutoML, we received an accuracy score of 0.9163 within just 5 iterations. 
<br>
![alt text](https://github.com/himanshu004/AZMLND_Optimizing_a_Pipeline_in_Azure-Starter_Files/blob/master/images/automl-metrics.jpg)
<br>
![alt text](https://github.com/himanshu004/AZMLND_Optimizing_a_Pipeline_in_Azure-Starter_Files/blob/master/images/automl-eval.jpg)

## Scikit-learn Pipeline

Steps involved in the entry script(train.py):
<ol>
<li>Creation of TabularDataset using TabularDatasetFactory.</li>
[Find dataset here](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv)

<li>Cleaning the data - removing rows with missing entries, one hot encoding the categorical data, feature engineering etc.</li>
<li>Splitting the data into train and test sets.</li>
<li>Training the logistic regression model using arguments from the HyperDrive runs.</li>
 <li>Calculating the accuracy score.</li>
</ol>
<br>
Steps involved in the project notebbok(udacity-project.ipynb):
<ol>
<li>Assigning a compute cluster to be used as the target.</li>
<li>Specifying the parameter sampler(RandomParameterSampling in this project).</li>
<li>Specifying an early termination policy(BanditPolicy in this project).</li>
<li>Creating a SKLearn estimator for use with train.py.</li>
<li>Creating a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.</li>
<li>Submitting  the hyperdrive run to the experiment and showing run details with the widget.</li>
<li>Getting the best run id and saving the model from that run.</li>
<li>Saving the model under the workspace for deployment.</li>
</ol>
<br>

**Benefits of the parameter sampler:**
RandomParameterSampling supports discrete and continous hyperparameters. In random sampling, hyperparameter values are chosen randomly, thus saving a lot of computational efforts. 

**Benefits of the early stopping policy:**
An early termination policy is quite helpful when the run becomes exhaustive. This early termination policy is based on the slack factor and delay evaluation. 

## AutoML
**Description of  the model and hyperparameters generated by AutoML:**
With number of iterations set to 5, AutoML generated LightGBM, XGBoostClassifier, RandomForest, VotingEnsemble and StackEnsemble models with VotingEnsemble giving the best accuracy score.

## Pipeline comparison

**Comparison between the two models and their performance on the basis of the differences in accuracy, architecture:**
Though both the models used automated machine learning somehow, a difference in the accuracies was visible, with model trained using AutoML gave slightly better results. Apart from that, an entry script was not a part of the architecture of the AutoML config.

**Reason of difference:**
The reason in accuracies might be due to the fact that that we used less number of iterations in AutoML run, which might give better results with more iterations. However, the difference was quite small.

## Future work
**Some areas of improvement for future experiments and why might these improvements help the model:**
Inferencing statistical insights from the data and feature engineering on it might be a scope of improvement. Improving the training data might give better results.


